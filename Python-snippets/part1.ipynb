{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1\n",
        "\n",
        "**Querying Data from a DB into a Pandas DataFrame or a CSV file**"
      ],
      "metadata": {
        "id": "aSF_GRhjAhwB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt1p5MZMAbqx"
      },
      "outputs": [],
      "source": [
        "import pyodbc\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#### DATABASE CREDENTIALS ####\n",
        "server = 'SERVERNAME'\n",
        "database = 'DATABASENAME'\n",
        "username = 'USER'\n",
        "password = 'PASSWORD'\n",
        "conn = pyodbc.connect('DRIVER={DRIVER NAME WITH BRACKETS INCLUDED};SERVER='+server+';\n",
        "\t\t      DATABASE='+database+';UID='+username+';PWD='+ password)\n",
        "conn_str = 'DRIVER={DRIVER NAME WITH BRACKETS INCLUDED};SERVER='+server+';\n",
        "\t\t      DATABASE='+database+';UID='+username+';PWD='+ password\n",
        "cursor = conn.cursor()\n",
        "\n",
        "\n",
        "\n",
        "#### CREATE A FUNCTION THAT FETCHES DATA USING AN SQL QUERY AND RETURNS IT IN A PYTHON LIST ####\n",
        "def run_sql(sql):\n",
        "\tsql = sql\n",
        "\twith pyodbc.connect(conn_str) as conn:\n",
        "\t\tcursor = conn.cursor()\n",
        "\t\tconn.autocommit = True\n",
        "\t\tcursor.execute(sql)\n",
        "\t\treturn [list(row) for row in cursor.fetchall()]\n",
        "\t\tcursor.close()\n",
        "\t\tdel cursor\n",
        "\n",
        "\n",
        "#### WRITE YOUR SQL ####\n",
        "sql = ''' WRITE YOUR SQL HERE ''''\n",
        "\n",
        "\n",
        "#### USE THE FUNCTION TO PUT YOUR QUERY DATA INTO A LIST ####\n",
        "\n",
        "data = db.run_sql(sql)\n",
        "\n",
        "#### IF YOU WANT IT IN A PANDAS DATAFRAME DO THIS ####\n",
        "\n",
        "#### Write the list of lists into a CSV file\n",
        "write_file = '~/directory/filename.csv'\n",
        "with open(write_file,'w') as f:\n",
        "    writer = csv.writer(f, delimiter = ',', lineterminator='\\n')\n",
        "    writer.writerows(data)\n",
        "\n",
        "\n",
        "#### Create a list of column names based on your data source\n",
        "names = ['column 1','column2','column3',[n]...]\n",
        "\n",
        "#### Read the file into a DataFrame\n",
        "df = pd.read_csv(write_file, names = names, usecols=names)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When SQL isn’t sufficient for an analysis or a complex data transformation, Python is probably the answer. But before you can wrangle any data, you have to get the data into memory so you can do stuff with it. Using PYODBC if you’re database is on MS SQL Server or PSYCOPG2 if you’re on Postgres, you can write queries and pull data easily using Python. From there it’s just a matter of getting your data into a format that’s easy to work with. For this I like Pandas. Getting the query data into Pandas is as simple as converting the list to a CSV and then using the pandas read_csv function. Alternatively, you could build the read_csv right into your run_sql function if desired."
      ],
      "metadata": {
        "id": "2W-uOV0XAuiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sending email alerts with the full error code output**"
      ],
      "metadata": {
        "id": "mELUB7NBA0E8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import smtplib\n",
        "import smtplib\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.base import MIMEBase\n",
        "from email import encoders\n",
        "import traceback\n",
        "\n",
        "#### THIS IS THE FUNCTION THAT WILL USE SMTPLIB TO SEND EMAILS VIA PYTHON ####\n",
        "def send_alert(subject,msg,receivers):\n",
        "\tmsg = MIMEText(msg,'html')\n",
        "\t# if '@microsoft.graph.downloadUrl' in files.keys():\n",
        "\tmsg['Subject'] = subject\n",
        "\tmsg['From'] = USER\n",
        "\tmsg['To'] = ','.join(receivers)\n",
        "\t# Send the message via our own SMTP server.\n",
        "\ttry:\n",
        "\t\ts = smtplib.SMTP('smtp.gmail.com','587')\n",
        "\t\ts.starttls()\n",
        "\t\ts.login(USER,PASSWORD)\n",
        "\t\ts.send_message(msg)\n",
        "\t\ts.quit()\n",
        "\texcept smtplib.SMTPException as e:\n",
        "\t\tprint(\"Error: unable to send email: {}\".format(e))\n",
        "\n",
        "#### INPUT THE EMAIL USER NAME AND PASSWORD AND WHO SHOULD RECEIVE THE EMAIL HERE ####\n",
        "USER = 'USER'\n",
        "PASSWORD = 'password'\n",
        "RECIEVERS = ['reciever1','reciever2',[n]....]\n",
        "\n",
        "\n",
        "#### THIS IS YOUR MAIN CODE CHUNK OR FUNCTION WITH TRY AND EXCEPT CRITERIA\n",
        "try:\n",
        "   some_function()\n",
        "except Exception as e:\n",
        "  tb = traceback.format_exc()\n",
        "  subject = 'There was an error on the script'\n",
        "  msg = f'Here is the full traceback: {tb}'\n",
        "  send_alert(subject,msg,RECIEVERS)"
      ],
      "metadata": {
        "id": "nhSODXkWA7dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monitoring errors in automated jobs (cron or other) is essential to running data pipelines or other code. I utilize email alerting extensively to alert me when any script I have running on an automated cadence breaks.\n",
        "\n",
        "To quickly get to the source of an issue I also really like to get the full traceback error message from Python in my email so I know exactly what to look for when I go to fix the script. The traceback package allows you to get this using traceback.format_exc() and then place that as a string in your email message.\n",
        "\n",
        "The best way to use the code snippet is actually to call the functions into another script where you have the code that you want to monitor. Wrap your code in a try/except statement and then upon exception execute the send_alert function to send a full error report to your email.\n",
        "\n",
        "Before using this code you need to have a dummy email account setup. Gmail is the simplest. For this example, make sure that you have ‘Less Secure App access turned on’"
      ],
      "metadata": {
        "id": "J4Eu59AtA-TZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Writing a CSV file into a DB (Postgres or SQL Server)**"
      ],
      "metadata": {
        "id": "83zTitZ7BDVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyodbc\n",
        "from azure.storage.blob import BlockBlobService\n",
        "\n",
        "#### These are your Azure Blob storage credentials ####\n",
        "AZB_CREDS = {\n",
        "\t'STORAGEACCOUNTNAME': \"ACCOUNT NAME\",\n",
        "\t'STORAGEKEY': \"###################\",\n",
        "\t'CREDENTIAL': 'AzureStorageCredential',\n",
        "\t'FILEFORMAT': 'generic_csv_format', #\n",
        "}\n",
        "\n",
        "\n",
        "#### This function creates an external data source on your database that points to blob storage ####\n",
        "def asdb_create_external_data_source(table_name, container_name):\n",
        "\tq_extds = '''\n",
        "\tCREATE EXTERNAL DATA SOURCE {table_name}_asdb_storage\n",
        "\tWITH\n",
        "\t(\n",
        "\t    TYPE = BLOB_STORAGE,\n",
        "\t    LOCATION='https://{blob_address_here}/{container_name}'\n",
        "\t)\n",
        "\t'''.format(table_name=table_name,container_name=container_name)\n",
        "\t# LOCATION ='wasbs://{container_name}@{storage_account_name}.blob.core.windows.net',\n",
        "\twith pyodbc.connect(conn_str) as conn:\n",
        "\t\tcursor = conn.cursor()\n",
        "\t\tconn.autocommit = True\n",
        "\t\ttry:\n",
        "\t\t\tcursor.execute(q_extds)\n",
        "\t\t\tprint('External data source created for the following container: {}'.format(container_name))\n",
        "\t\texcept Exception as e:\n",
        "\t\t\t# print 'Error with creating external data source for the following container: {} \\nHere is the error: {}'.format(container_name,e)\n",
        "\t\t\tpass\n",
        "\t\tcursor.close()\n",
        "\t\tdel cursor\n",
        "\t\tconn.autocommit = False\n",
        "\n",
        "\n",
        "#### This function drops a temporary version of a table with the string _tmp at the end ####\n",
        "def drop_temp_table(schema, table):\n",
        "\tsql = '''\n",
        "\tDROP TABLE {schema}.{table}_tmp;\n",
        "\t'''.format(table=table, schema=schema)\n",
        "\twith pyodbc.connect(conn_str) as conn:\n",
        "\t\tcursor = conn.cursor()\n",
        "\t\tconn.autocommit = True\n",
        "\t\ttry:\n",
        "\t\t\tcursor.execute(sql)\n",
        "\t\t\tprint('Table {schema}.{table}_tmp dropped.'.format(schema=schema,table=table))\n",
        "\t\texcept Exception as e:\n",
        "\t\t\t# print 'Error dropping external table dbo.{}\\nError: {}'.format(table_name,e) # errors when the table doesn't exist - all good\n",
        "\t\t\tpass\n",
        "\t\tcursor.close()\n",
        "\t\tdel cursor\n",
        "\t\tconn.autocommit = False\n",
        "\n",
        "\n",
        "#### This function creates a temporary table from the target table you want to push data to ####\n",
        "def create_temp_table(schema,table_name):\n",
        "\twith pyodbc.connect(conn_str) as conn:\n",
        "\t\tcursor = conn.cursor()\n",
        "\t\tconn.autocommit = True\n",
        "\t\tsql = '''\n",
        "\t\tselect *\n",
        "\t\tinto {schema}.{table_name}_tmp\n",
        "\t\tfrom {schema}.{table_name}\n",
        "\t\twhere 1 = 0;\n",
        "\t\t'''.format(schema=schema,table_name=table_name)\n",
        "\t\t# print sql\n",
        "\t\tcursor.execute(sql)\n",
        "\t\tprint('Created temp table [{}].[{}]'.format(schema,table_name+'_tmp'))\n",
        "\t\tcursor.close()\n",
        "\t\tdel cursor\n",
        "\t\tconn.autocommit = False\n",
        "\n",
        "#### This function inserts a csv file from blob storage into the target table ####\n",
        "def bulk_insert(schema, table_name, data_source_name, container_file_name, field_terminator=',', row_terminator='0x0a'):\n",
        "    sql = '''\n",
        "    BULK INSERT {schema}.{table_name}\n",
        "    FROM '{container_file_name}'\n",
        "    WITH (DATA_SOURCE = '{data_source_name}',ROWTERMINATOR='{row_terminator}',FIELDTERMINATOR='{field_terminator}', FORMAT='CSV', CODEPAGE = '65001')\n",
        "    '''.format(schema=schema,table_name=table_name\n",
        "      ,container_file_name=container_file_name,data_source_name=data_source_name\n",
        "      ,field_terminator=field_terminator,row_terminator=row_terminator)\n",
        "    # print(sql)\n",
        "    with pyodbc.connect(conn_str) as conn:\n",
        "      cursor = conn.cursor()\n",
        "      conn.autocommit = True\n",
        "      message = 'Success'\n",
        "      try:\n",
        "        cursor.execute(sql)\n",
        "      except Exception as e:\n",
        "        print('Error with bulk insert: {} \\nHere is the error: {}'.format(container_file_name,e))\n",
        "        message = 'Error with bulk insert: {} \\nHere is the error: {}'.format(container_file_name,e)\n",
        "\n",
        "    cursor.close()\n",
        "    del cursor\n",
        "    conn.autocommit = False\n",
        "    return message\n",
        "\n",
        "\n",
        "\n",
        "#### This function merges the temporary table with the main table using a list of unique keys ####\n",
        "def merge_temp_table_with_main(schema,table_name,list_of_unique_fields):\n",
        "  tbl = '{schema}.{table_name}'.format(schema=schema,table_name=table_name)\n",
        "  tmp_tbl = tbl+'_tmp'\n",
        "  unique_where_filter_string = 'AND '.join(['coalesce({tbl}.{col},\\'NULL\\') = coalesce({tmp_tbl}.{col},\\'NULL\\')\\n'.format(col=c,tbl=tbl,tmp_tbl=tmp_tbl) for c in list_of_unique_fields])\n",
        "  sql = '''\n",
        "  delete from {schema}.{table_name}\n",
        "  where exists (select 1 from {schema}.{table_name}_tmp where {unique_where_filter_string});\n",
        "  insert into {schema}.{table_name} select * from {schema}.{table_name}_tmp;\n",
        "  drop table {schema}.{table_name}_tmp;\n",
        "  '''.format(schema=schema,table_name=table_name\n",
        "    ,unique_where_filter_string=unique_where_filter_string)\n",
        "  #print sql\n",
        "  with pyodbc.connect(conn_str) as conn:\n",
        "    cursor = conn.cursor()\n",
        "    conn.autocommit = True\n",
        "    try:\n",
        "      cursor.execute(sql)\n",
        "      print('Successful merge into {}.{}.'.format(schema,table_name))\n",
        "      message = 'Successful merge into {}.{}.'.format(schema,table_name)\n",
        "    except Exception as e:\n",
        "      print('Error with the merge into {}.{}\\nError: {}'.format(schema,table_name,e))\n",
        "      # pass\n",
        "      message = 'Error with the merge into {}.{}\\nError: {}'.format(schema,table_name,e)\n",
        "    cursor.close()\n",
        "    del cursor\n",
        "    conn.autocommit = False\n",
        "  return message\n",
        "\n",
        "\n",
        "write_file = '~/directory/file.csv'\n",
        "\n",
        "### STEP 1: Create a new blob in your container of choice in azure blob storage using your csv file. This function uses the create_blob_from_path function from the azure blob api\n",
        "container_name = 'NAME OF YOUR CONTAINER'\n",
        "container_file_name = 'file.csv'.format(today)\n",
        "blob_service = BlockBlobService(account_name=AZB_CREDS['STORAGEACCOUNTNAME'], account_key=AZB_CREDS['STORAGEKEY'])\n",
        "\n",
        "blob_service.create_blob_from_path(container_name, container_file_name, write_file) # this always overwrites whatever is in the container\n",
        "print('Done sending txt file up to AZB ...')\n",
        "\n",
        "### STEP 2: Create an external data source that ties a specified database table to a container in blob storage\n",
        "target_schema = 'schema'\n",
        "table_name = 'table'\n",
        "data_source_name = table_name+'_asdb_storage' # you can create this on the next line\n",
        "asdb_create_external_data_source(table_name, container_name)\n",
        "\n",
        "### STEP 3: If a temporary table already exists, drop it. Then create a new temporary table to push the CSV file to\n",
        "drop_temp_table(target_schema, table_name)\n",
        "create_temp_table(target_schema, table_name)\n",
        "\n",
        "### STEP 4: Insert the CSV file from blob storage into the temporary table\n",
        "bulk_insert(target_schema,table_name+'_tmp', data_source_name, container_file_name)\n",
        "\n",
        "### STEP 5: Merge the CSV data into the existing table using merge criteria from the temporary table\n",
        "unique_columns = ['column1','column2','column3',[n].....]\n",
        "merge_temp_table_with_main(target_schema,table_name,unique_columns)"
      ],
      "metadata": {
        "id": "oXuVpN8QBGIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is A LOT of code — but it is one of the most important techniques that I use on a daily basis to get data from any source and push it into a database. The example above is best suited for working with MS SQL Server — specifically Azure SQL Database and Azure Blob Storage. Below the another example that you can use for POSTGRES databases — which is a bit simpler.\n",
        "\n",
        "The basic idea here is to get the data from a CSV file into your database using a few steps:\n",
        "\n",
        "1. Push the CSV file into blob storage.\n",
        "2. Tie the target database table to blob storage by creating an external data source on your SQL database.\n",
        "3. Create a temporary table where you will insert the CSV file.\n",
        "4. Bulk insert the CSV file into the temporary table\n",
        "5. Finally, merge the temporary table into the final table using merge criteria.\n",
        "\n",
        "There are countless ways to get data where you want it to go. This is the method that I depend on almost every day."
      ],
      "metadata": {
        "id": "QIGWqlgdBP2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for the postgres databases\n",
        "import psycopg2\n",
        "\n",
        "HOST = 'HOSTNAME'\n",
        "DB = 'DBNAME'\n",
        "USER = 'USERNAME'\n",
        "PASSWORD = 'PASSWORD'\n",
        "PORT = 'PORT'\n",
        "\n",
        "conn = psycopg2.connect(host=HOST,database=DB,user=USER,password=PASSWORD,port=PORT)\n",
        "\n",
        "# cursor = conn.cursor()\n",
        "\n",
        "# this can bulk insert a file into a table\n",
        "def copy_file_into(schema, table, file, delimiter=',',null='NULL',columns=[]):\n",
        "    cursor = conn.cursor()\n",
        "    fpath = open(file,'r')\n",
        "    if len(columns) > 0:\n",
        "        cols_str = ','.join(columns)\n",
        "        copy_sql = f'''COPY {schema}.{table} ({cols_str}) FROM STDIN WITH CSV DELIMITER E'{delimiter}' QUOTE '\"' NULL '{null}';''' # allows there to be embedded commas which in NICE!!\n",
        "    else:\n",
        "        copy_sql = f'''COPY {schema}.{table} FROM STDIN WITH CSV DELIMITER E'{delimiter}' QUOTE '\"' NULL '{null}';''' # allows there to be embedded commas which in NICE!!\n",
        "    # copy_sql = f'''COPY {schema}.{table} FROM STDIN WITH CSV DELIMITER E'\\t' QUOTE '\"' NULL 'NULL';''' # allows there to be embedded commas which in NICE!!\n",
        "    print(copy_sql)\n",
        "    cursor.copy_expert(copy_sql, fpath)\n",
        "    print('Done with the copy')\n",
        "    cursor.execute('COMMIT;')\n",
        "    cursor.close()\n",
        "    del cursor\n",
        "\n",
        "# use this to merge data from one table to another - you just need to define the sources, targets and primary key fields\n",
        "def merge_one_table_with_main(source_schema,target_schema,source_table,target_table,list_of_unique_fields):\n",
        "    # unique_where_filter_string = 'AND '.join(['{target_schema}.{target_table}.{col} = {source_schema}.{source_table}.{col}\\n'.format(col=c,target_table=target_table,source_table=source_table,source_schema=source_schema,target_schema=target_schema) for c in list_of_unique_fields])\n",
        "    unique_where_filter_string = 'AND '.join([f'{target_schema}.{target_table}.{col} = {source_schema}.{source_table}.{col}\\n' for col in list_of_unique_fields])\n",
        "    sql = f'''\n",
        "    delete from {target_schema}.{target_table}\n",
        "    where exists (select 1 from {source_schema}.{source_table} where {unique_where_filter_string});\n",
        "    insert into {target_schema}.{target_table} select * from {source_schema}.{source_table};\n",
        "    drop table {source_schema}.{source_table};\n",
        "    COMMIT;'''\n",
        "    # print(sql)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(sql)\n",
        "    cursor.close()\n",
        "    del cursor\n",
        "\n",
        "\n",
        "def create_temp_table(schema, table, temp_table):\n",
        "   sql = f'''\n",
        "   DROP TABLE IF EXISTS {schema}.{temp_table};\n",
        "   SELECT * INTO {schema}.{temp_table} FROM {schema}.{table} WHERE 1 = 0;\n",
        "   COMMIT;\n",
        "   '''\n",
        "   cursor = conn.cursor()\n",
        "   cursor.execute(sql)\n",
        "   cursor.close()\n",
        "   del cursor"
      ],
      "metadata": {
        "id": "S06He3T8BsM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pulling back multiple pages of data from a REST API**"
      ],
      "metadata": {
        "id": "r6irepRIBxeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import csv\n",
        "\n",
        "\n",
        "\n",
        "#### PULLING FROM A SYNCHRONOUS ENDPOINT\n",
        "\n",
        "endpoint = '#######'\n",
        "url = f'https://place.com/v11.0/{endpoint}/insights'\n",
        "\tr = requests.get(url,params=params)\n",
        "\tapiResults = r.json()\n",
        "\n",
        "\n",
        "data = []\n",
        "\n",
        "while(True):\n",
        "\ttry:\n",
        "\t\tfor row in apiResults['data']:\n",
        "\t\t\tdata.append(row)\n",
        "\t\t# Attempt to make a request to the next page of data, if it exists.\n",
        "\t\tapiResults=requests.get(apiResults['paging']['next']).json()\n",
        "\texcept KeyError:\n",
        "\t\t# When there are no more pages (['paging']['next']), break from the\n",
        "\t\t# loop and end the script.\n",
        "\t\tbreak\n",
        "\n",
        "#### PULLING FROM A ASYNCHRONOUS ENDPOINT (REQUEST IS PROCESSED IN THE BACKGROUND)\n",
        "\n",
        "endpoint = '#######'\n",
        "url = f'https://place.com/v11.0/{endpoint}'\n",
        "r = requests.post(url,params=params)\n",
        "if r.status_code != 200:\n",
        "\tprint(f'{r.status_code} error with request: {r.text}\\nExiting the program ...')\n",
        "\traise SystemExit\n",
        "apiResults = r.json()\n",
        "print(apiResults)\n",
        "\n",
        "report_run_id = campaigns.get('report_run_id')\n",
        "job_status = None\n",
        "\n",
        "while job_status != 'Job Completed':\n",
        "\n",
        "\t\tparams = {\n",
        "\t            'access_token':'############'\n",
        "\t        }\n",
        "\n",
        "\t\turl = f'https://place.com/v11.0/{report_run_id}'\n",
        "\t\tr = requests.get(url,params=params)\n",
        "\t\tapiResults = r.json()\n",
        "\t\tjob_status = apiResults.get('status')\n",
        "\t\tif job_status == 'Job Completed':\n",
        "\t\t\tprint('Job completed...breaking the loop and retrieving the data')\n",
        "\t\t\tbreak\n",
        "\t\telse:\n",
        "\t\t\tprint('Job not completed....percent completed: {}'.format(results.get('percent_completion')))\n",
        "\t\t\ttime.sleep(30)\n",
        "\n",
        "\n",
        "url = f'https://place.com/v11.0/{report_run_id}'\n",
        "r = requests.get(url,params=params)\n",
        "apiResults = r.json()"
      ],
      "metadata": {
        "id": "u02qxH9PB8-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whenever I need to create an ETL pipeline with an API integration, I will use some iteration of the above two pieces of code. A lot of APIs use asynchronous requests (where you make a code request and then wait for it to complete in the background), which I also included in the example. To get the code to work, you’ll need to swap out the endpoint URLs and the specific pagination key for the API that you’re using. If the API requires authentication using a client_id/client_secret then you’ll also need to follow an authentication process to acquire an access token. Once you’ve done that, the above code examples will serve as a great starting point to get your integration built."
      ],
      "metadata": {
        "id": "3sF26RltCBL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cleaning and formatting REST API JSON results for database insertion**"
      ],
      "metadata": {
        "id": "qZkc0eWHCJvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "##### API URL and URL PARAMETERS GO HERE #####\n",
        "params = {'######':'######'}\n",
        "url = 'request_url'\n",
        "\n",
        "\n",
        "##### CREATE AN EMPTY LIST TO STORE THE JSON DATA #####\n",
        "table = []\n",
        "\n",
        "##### MAKE THE REQUEST AND THEN PUT THE REQUEST DATA INTO JSON FORMAT #####\n",
        "r = requests.get(url, params=params)\n",
        "data = r.json()\n",
        "\n",
        "##### LOOP THROUGH EACH JSON OBJECT IN THE LIST AND PUT IT CLEANLY INTO ITS OWN ROW  #####\n",
        "for item in data['key_where_the_data_lives']:\n",
        "  ### item.keys() (to figure out what the keys are)\n",
        "  row = [\n",
        "  item.get('key1'),\n",
        "  item.get('key2'),\n",
        "  item.get('key3'),\n",
        "  item.get('key4'),\n",
        "  item.get('key5'),\n",
        "  item.get('keyN')\n",
        "  ]\n",
        "  table.append(row)\n",
        "\n",
        "\n",
        "##### WRITE THE ROWS OF THE LIST (TABLE) INTO A CSV FILE ######\n",
        "write_file = '~/directory/filename.csv'\n",
        "with open(write_file,'w', newline = '') as f:\n",
        "    writer = csv.writer(f, lineterminator = '\\n')\n",
        "    writer.writerows(table)"
      ],
      "metadata": {
        "id": "02yUTmL_CLnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I use a variation on the above code snippet for almost any API integration that I write. This one assumes that the data will come back in JSON format — meaning it should work for about 75% of API requests for data. When formatted, this script will organize a JSON data structure into a tabular format (list of lists) which can then be easily inserted into a CSV — making it ready for pushing to a DB. There’s probably a much simpler way to do this but this is the method I’ve been using for years and it has yet to fail me."
      ],
      "metadata": {
        "id": "fje3-XvfCRe4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cleaning a CSV file for insertion into a DB**"
      ],
      "metadata": {
        "id": "66A7_HDtCTNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "write_file = '~/directory/filename.csv'\n",
        "df = pd.read_csv(write_file)\n",
        "df = df.replace({‘\\$’:’’}, regex = True)\n",
        "df = df.replace({‘%’:’’}, regex = True)\n",
        "df = df.replace({‘\\,’:’’}, regex = True)\n",
        "df = df.replace({“‘“:’’}, regex = True)\n",
        "df = df.replace({“nan”:’’}, regex = True)"
      ],
      "metadata": {
        "id": "cJyV3KIqCVDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating To and From Dates**"
      ],
      "metadata": {
        "id": "wClo5kdeCZBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "#### Set todays date ####\n",
        "today = datetime.datetime.now().date()\n",
        "#### Create to_date by adding or subtracting dates from today's date\n",
        "to_date = today-datetime.timedelta(1)\n",
        "#### Create your from date by subtracting the number of days back #### you want to start\n",
        "from_date = today-datetime.timedelta(7)\n",
        "#### Create timestamp of today's date using desired format\n",
        "todaysdate = datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d')"
      ],
      "metadata": {
        "id": "dW_AkBzACaHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most API’s require date parameters when requesting data. This example uses Python’s datetime package to easily create to and from dates using the .timedelta() function. I use this all the time to build automatic to/from date calculations into my script based on the day/time that the script is running."
      ],
      "metadata": {
        "id": "x_oFmc-4CgbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Process a big Python list in chunks**"
      ],
      "metadata": {
        "id": "ZrbcPg5tChkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunker(seq, size):\n",
        " return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
        "for group in chunker(order_items,100):\n",
        "      for item in group:\n",
        "# Do something to each group"
      ],
      "metadata": {
        "id": "w2FRFcwCCjFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a little function I use quite a bit to chunk up really big lists. This goes really will with cleaning and formatting JSON data (shown above) as sometimes you’ll be pulling back a lot of data from an API and want to get it processed and pushed into your DB in manageable chunks. The chunker function allows you to pick how large of chunks you want to process at a time, and then run your code over each chunk. This example uses a 100 item chunk, but you can adjust that number to be whatever you want."
      ],
      "metadata": {
        "id": "1o6Mu7UJCntA"
      }
    }
  ]
}