{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Step one:** collect data using web scraping and API.\n",
        "\n",
        "Identified the website with the data that about the population of the cities where Gans is active in order to see how many people are on target with the company, and then collected data for Barcelona, Paris, and Berlin.\n",
        "\n",
        "The most popular and available [website](https://ugeo.urbistat.com/AdminStat/en/es/demografia/eta/barcelona/8/3)."
      ],
      "metadata": {
        "id": "cin4B00y5VHS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqp_PLPs5Gmn"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://ugeo.urbistat.com/AdminStat/en/es/demografia/eta/barcelona/8/3\"\n",
        "response = requests.get(url)\n",
        "response.status_code\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")"
      ],
      "metadata": {
        "id": "oa5Vm2ET6avX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected the div and the classes to be sure to get only these pieces of information."
      ],
      "metadata": {
        "id": "nqO3gIXf6nn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the HTML table with the class \"table_result_light\" and store it in 'table'\n",
        "table = soup.select_one(\"table.table_result_light\")\n",
        "\n",
        "age_range = []\n",
        "\n",
        "for i in range(len(table.select('td.table_label'))):\n",
        "    # Append the text content of each td element to the 'age_range' list\n",
        "    age_range.append(table.select('td.table_label')[i].get_text())\n",
        "\n",
        "# Create a DataFrame from the 'age_range' list with column name 'age_range'\n",
        "age_range = pd.DataFrame({\"age_range\": age_range})\n",
        "\n",
        "# Clean the 'age_range' column by removing unwanted characters using regex\n",
        "age_range.replace(to_replace = [r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"],\n",
        "                  value = [\"\",\"\"],\n",
        "                  regex=True,\n",
        "                  inplace=True)\n",
        "values=[]\n",
        "\n",
        "for i in range(len(table.select(\"td\"))):\n",
        "    values.append(table.select(\"td\")[i].get_text())\n",
        "\n",
        "# Create a DataFrame from the 'values' list with column name 'value'\n",
        "values = pd.DataFrame({\"value\":values})\n",
        "\n",
        "# Clean the 'value' column by removing unwanted characters using regex\n",
        "values.replace(to_replace = [r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"],\n",
        "               value=[\"\",\"\"],\n",
        "               regex=True,\n",
        "               inplace=True)\n",
        "\n",
        "# Further clean the 'value' column by removing commas and empty values\n",
        "values.replace(to_replace=[\",\", \"\"],\n",
        "               value=[\"\",\"\"],\n",
        "               regex=True,\n",
        "               inplace=True)\n",
        "\n",
        "# Extract specific rows from the 'values' DataFrame to get total population data\n",
        "# value = the number of people for each specific range\n",
        "tot_pop = values.loc[[9,16,23,30,37,44,51,58,65,72,79]]\n",
        "\n",
        "# Add a new column 'perc' to 'tot_pop' DataFrame with corresponding values\n",
        "# perc = the percentage in relation to the total population,\n",
        "tot_pop['perc'] = [2,3,6,6,7,12,16,16,12,10,9]\n",
        "\n",
        "# Add a new column 'city' to 'tot_pop' DataFrame with the value 'barcelona'\n",
        "tot_pop['city']='barcelona'\n",
        "\n",
        "# Reset the index of 'tot_pop' DataFrame and drop the old index\n",
        "tot_pop = tot_pop.reset_index()\n",
        "tot_pop = tot_pop.drop('index', axis = 1)\n",
        "\n",
        "# Combine 'age_range' and 'tot_pop' DataFrames with an inner join on common keys\n",
        "population_barcelona = age_range.join(tot_pop, how=\"inner\")\n",
        "\n",
        "# Add a new column 'city_iso' to 'population_barcelona' DataFrame with the value 'barcelona_ES'\n",
        "population_barcelona['city_iso'] = 'barcelona_ES'"
      ],
      "metadata": {
        "id": "2SolEdJN6niY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "population_barcelona.head()"
      ],
      "metadata": {
        "id": "8LtKmggZ_cua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then repeat the process for Paris\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zmH8G8Nu_58t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://ugeo.urbistat.com/AdminStat/en/fr/demografia/eta/paris/75/3\"\n",
        "response = requests.get(url)\n",
        "response.status_code\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")"
      ],
      "metadata": {
        "id": "qqQIYl_uAQN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the HTML table with the class \"table_result_light\" and store it in 'table'\n",
        "table = soup.select_one(\"table.table_result_light\")\n",
        "\n",
        "age_range = []\n",
        "\n",
        "for i in range(len(table.select('td.table_label'))):\n",
        "    # Append the text content of each td element to the 'age_range' list\n",
        "    age_range.append(table.select('td.table_label')[i].get_text())\n",
        "\n",
        "# Create a DataFrame from the 'age_range' list with column name 'age_range'\n",
        "age_range = pd.DataFrame({\"age_range\": age_range})\n",
        "\n",
        "# Clean the 'age_range' column by removing unwanted characters using regex\n",
        "age_range.replace(to_replace = [r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"],\n",
        "                  value = [\"\",\"\"],\n",
        "                  regex=True,\n",
        "                  inplace=True)\n",
        "values=[]\n",
        "\n",
        "for i in range(len(table.select(\"td\"))):\n",
        "    values.append(table.select(\"td\")[i].get_text())\n",
        "\n",
        "# Create a DataFrame from the 'values' list with column name 'value'\n",
        "values = pd.DataFrame({\"value\":values})\n",
        "\n",
        "# Clean the 'value' column by removing unwanted characters using regex\n",
        "values.replace(to_replace = [r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"],\n",
        "               value=[\"\",\"\"],\n",
        "               regex=True,\n",
        "               inplace=True)\n",
        "\n",
        "# Further clean the 'value' column by removing commas and empty values\n",
        "values.replace(to_replace=[\",\", \"\"],\n",
        "               value=[\"\",\"\"],\n",
        "               regex=True,\n",
        "               inplace=True)\n",
        "\n",
        "# Extract specific rows from the 'values' DataFrame to get total population data\n",
        "# value = the number of people for each specific range\n",
        "tot_pop = values.loc[[9,16,23,30,37,44,51,58,65,72,79]]\n",
        "\n",
        "# Add a new column 'perc' to 'tot_pop' DataFrame with corresponding values\n",
        "# perc = the percentage in relation to the total population\n",
        "tot_pop['perc'] = [2,3,6,6,7,12,16,16,12,10,9]\n",
        "\n",
        "# Add a new column 'city' to 'tot_pop' DataFrame with the value 'paris'\n",
        "tot_pop['city']='paris'\n",
        "\n",
        "# Reset the index of 'tot_pop' DataFrame and drop the old index\n",
        "tot_pop = tot_pop.reset_index()\n",
        "tot_pop = tot_pop.drop('index', axis = 1)\n",
        "\n",
        "# Combine 'age_range' and 'tot_pop' DataFrames with an inner join on common keys\n",
        "population_paris = age_range.join(tot_pop, how = \"inner\")\n",
        "\n",
        "# Add a new column 'city_iso' to 'population_paris' DataFrame with the value 'paris_FR'\n",
        "population_paris['city_iso'] = 'paris_FR'"
      ],
      "metadata": {
        "id": "t9h0CgHBAUL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "population_paris.head()"
      ],
      "metadata": {
        "id": "M1FqdrdDBXDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And for Berlin"
      ],
      "metadata": {
        "id": "hh52NuRLB1OW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://ugeo.urbistat.com/AdminStat/en/de/demografia/eta/berlin/11/2\"\n",
        "response = requests.get(url)\n",
        "response.status_code\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")"
      ],
      "metadata": {
        "id": "zyIfpYMdB3Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the HTML table with the class \"table_result_light\" and store it in 'table'\n",
        "table = soup.select_one(\"table.table_result_light\")\n",
        "\n",
        "age_range = []\n",
        "\n",
        "for i in range(len(table.select('td.table_label'))):\n",
        "    # Append the text content of each td element to the 'age_range' list\n",
        "    age_range.append(table.select('td.table_label')[i].get_text())\n",
        "\n",
        "# Create a DataFrame from the 'age_range' list with column name 'age_range'\n",
        "age_range = pd.DataFrame({\"age_range\": age_range})\n",
        "\n",
        "# Clean the 'age_range' column by removing unwanted characters using regex\n",
        "age_range.replace(to_replace = [r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"],\n",
        "                  value = [\"\",\"\"],\n",
        "                  regex=True,\n",
        "                  inplace=True)\n",
        "values=[]\n",
        "\n",
        "for i in range(len(table.select(\"td\"))):\n",
        "    values.append(table.select(\"td\")[i].get_text())\n",
        "\n",
        "# Create a DataFrame from the 'values' list with column name 'value'\n",
        "values = pd.DataFrame({\"value\":values})\n",
        "\n",
        "# Clean the 'value' column by removing unwanted characters using regex\n",
        "values.replace(to_replace = [r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"],\n",
        "               value=[\"\",\"\"],\n",
        "               regex=True,\n",
        "               inplace=True)\n",
        "\n",
        "# Further clean the 'value' column by removing commas and empty values\n",
        "values.replace(to_replace=[\",\", \"\"],\n",
        "               value=[\"\",\"\"],\n",
        "               regex=True,\n",
        "               inplace=True)\n",
        "\n",
        "# Extract specific rows from the 'values' DataFrame to get total population data\n",
        "# value = the number of people for each specific range\n",
        "tot_pop = values.loc[[9,16,23,30,37,44,51,58,65,72,79]]\n",
        "\n",
        "# Add a new column 'perc' to 'tot_pop' DataFrame with corresponding values\n",
        "# perc = the percentage in relation to the total population,\n",
        "tot_pop['perc'] = [2,3,6,6,7,12,16,16,12,10,9]\n",
        "\n",
        "# Add a new column 'city' to 'tot_pop' DataFrame with the value 'berlin'\n",
        "tot_pop['city'] = 'berlin'\n",
        "\n",
        "# Reset the index of 'tot_pop' DataFrame and drop the old index\n",
        "tot_pop = tot_pop.reset_index()\n",
        "tot_pop = tot_pop.drop('index', axis = 1)\n",
        "\n",
        "# Combine 'age_range' and 'tot_pop' DataFrames with an inner join on common keys\n",
        "population_berlin = age_range.join(tot_pop, how = \"inner\")\n",
        "\n",
        "# Add a new column 'city_iso' to 'population_berlin' DataFrame with the value 'berlin_ES'\n",
        "population_berlin['city_iso'] = 'berlin_DE'"
      ],
      "metadata": {
        "id": "4DBHzjCGB-b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "population_berlin.head()"
      ],
      "metadata": {
        "id": "NHmiIbjvCc43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "population_cities = population_paris.append([population_berlin,population_barcelona]).reset_index()\n",
        "population_cities = population_cities.drop('index', axis=1)\n",
        "population_cities['value'] = population_cities['value'].astype(int)"
      ],
      "metadata": {
        "id": "IY1xWwDK_fjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "population_cities.head(50)"
      ],
      "metadata": {
        "id": "gh5LR590C7R7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data with API**\n",
        "\n",
        "In addition to collecting data using web scraping, we can use the API(Application programming interface) which is a software intermediary that allows two applications to talk to each other.\n",
        "\n",
        "In our case we need data, they give us data. For the project, used two types of APIs: one to check the arrival flights in Berlin, Paris, and Barcelona and one to check the weather for the same cities."
      ],
      "metadata": {
        "id": "UY3HOKYcDEb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import JSON\n",
        "import requests"
      ],
      "metadata": {
        "id": "dJpjk40HDBgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "API from the RapidAPI"
      ],
      "metadata": {
        "id": "llK8CIv1Jixq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://aerodatabox.p.rapidapi.com/airports/icao/EDDB\"\n",
        "\n",
        "querystring = {\n",
        "    \"withLeg\": \"true\",\n",
        "    \"direction\": \"Arrival\",\n",
        "    \"withCancelled\": \"true\",\n",
        "    \"withCodeshared\": \"true\",\n",
        "    \"withCargo\": \"true\",\n",
        "    \"withPrivate\": \"true\",\n",
        "    \"withLocation\": \"true\"\n",
        "}\n",
        "\n",
        "headers = {\n",
        "\t\"X-RapidAPI-Key\": \"enter-the-key\",\n",
        "\t\"X-RapidAPI-Host\": \"aerodatabox.p.rapidapi.com\"\n",
        "}\n",
        "\n",
        "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "id": "FCXVVvlWN6bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "JSON(response.json())\n",
        "flight_data = response.json()\n"
      ],
      "metadata": {
        "id": "gZ91yyECSQob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#identify airport name\n",
        "flight_data['arrivals'][0]['departure']['airport']['name']\n",
        "# identify arrival time\n",
        "flight_data['arrivals'][0]['arrival']['scheduledTimeLocal']\n",
        "#identify airline name\n",
        "flight_data['arrivals'][0]['airline']['name']"
      ],
      "metadata": {
        "id": "BflMf0eYSV6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "departure=[]\n",
        "arrival_time=[]\n",
        "airline=[]\n",
        "for i in range(len(flight_data['arrivals'])):\n",
        "    departure.append(flight_data['arrivals'][i]['departure']['airport']['name'])\n",
        "    arrival_time.append(flight_data['arrivals'][i]['arrival']['scheduledTimeLocal'][10:-6])\n",
        "    airline.append(flight_data['arrivals'][i]['airline']['name'])\n",
        "#creation dataframe\n",
        "flight_data_berlin=pd.DataFrame({'city':'berlin',\n",
        "                                 'airport_departure':departure,\n",
        "                                 'airline':airline,\n",
        "                                 'arrival_time':arrival_time,\n",
        "                                 'city_iso':'berlin_DE'})"
      ],
      "metadata": {
        "id": "A-rS5OXT8laI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeating the same process for Paris"
      ],
      "metadata": {
        "id": "nYM4s5FOwV0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://aerodatabox.p.rapidapi.com/airports/icao/LFPG\"\n",
        "\n",
        "querystring = {\n",
        "    \"withLeg\": \"true\",\n",
        "    \"direction\": \"Arrival\",\n",
        "    \"withCancelled\": \"true\",\n",
        "    \"withCodeshared\": \"true\",\n",
        "    \"withCargo\": \"true\",\n",
        "    \"withPrivate\": \"true\",\n",
        "    \"withLocation\": \"true\"\n",
        "}\n",
        "\n",
        "headers = {\n",
        "\t\"X-RapidAPI-Key\": \"enter-the-key\",\n",
        "\t\"X-RapidAPI-Host\": \"aerodatabox.p.rapidapi.com\"\n",
        "}\n",
        "\n",
        "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
        "\n",
        "\n",
        "JSON(response.json())\n",
        "flight_data = response.json()\n",
        "\n",
        "\n",
        "#identify airport name\n",
        "flight_data['arrivals'][0]['departure']['airport']['name']\n",
        "# identify arrival time\n",
        "flight_data['arrivals'][0]['arrival']['scheduledTimeLocal']\n",
        "#identify airline name\n",
        "flight_data['arrivals'][0]['airline']['name']\n",
        "\n",
        "\n",
        "departure=[]\n",
        "arrival_time=[]\n",
        "airline=[]\n",
        "\n",
        "for i in range(len(flight_data['arrivals'])):\n",
        "    departure.append(flight_data['arrivals'][i]['departure']['airport']['name'])\n",
        "    arrival_time.append(flight_data['arrivals'][i]['arrival']['scheduledTimeLocal'][10:-6])\n",
        "    airline.append(flight_data['arrivals'][i]['airline']['name'])\n",
        "#creation dataframe\n",
        "flight_data_paris=pd.DataFrame({'city':'paris',\n",
        "                                 'airport_departure':departure,\n",
        "                                 'airline':airline,\n",
        "                                 'arrival_time':arrival_time,\n",
        "                                 'city_iso':'paris_FR'})\n"
      ],
      "metadata": {
        "id": "AlMc44vnwbVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same for barcelona"
      ],
      "metadata": {
        "id": "TC3CwfcbiPe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://aerodatabox.p.rapidapi.com/airports/icao/LEBL\"\n",
        "\n",
        "querystring = {\n",
        "    \"withLeg\": \"true\",\n",
        "    \"direction\": \"Arrival\",\n",
        "    \"withCancelled\": \"true\",\n",
        "    \"withCodeshared\": \"true\",\n",
        "    \"withCargo\": \"true\",\n",
        "    \"withPrivate\": \"true\",\n",
        "    \"withLocation\": \"true\"\n",
        "}\n",
        "\n",
        "headers = {\n",
        "\t\"X-RapidAPI-Key\": \"enter-the-key\",\n",
        "\t\"X-RapidAPI-Host\": \"aerodatabox.p.rapidapi.com\"\n",
        "}\n",
        "\n",
        "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
        "\n",
        "\n",
        "JSON(response.json())\n",
        "flight_data = response.json()\n",
        "\n",
        "\n",
        "#identify airport name\n",
        "flight_data['arrivals'][0]['departure']['airport']['name']\n",
        "# identify arrival time\n",
        "flight_data['arrivals'][0]['arrival']['scheduledTimeLocal']\n",
        "#identify airline name\n",
        "flight_data['arrivals'][0]['airline']['name']\n",
        "\n",
        "\n",
        "departure=[]\n",
        "arrival_time=[]\n",
        "airline=[]\n",
        "\n",
        "for i in range(len(flight_data['arrivals'])):\n",
        "    departure.append(flight_data['arrivals'][i]['departure']['airport']['name'])\n",
        "    arrival_time.append(flight_data['arrivals'][i]['arrival']['scheduledTimeLocal'][10:-6])\n",
        "    airline.append(flight_data['arrivals'][i]['airline']['name'])\n",
        "#creation dataframe\n",
        "flight_data_barcelona=pd.DataFrame({'city':'barcelona',\n",
        "                                 'airport_departure':departure,\n",
        "                                 'airline':airline,\n",
        "                                 'arrival_time':arrival_time,\n",
        "                                 'city_iso':'barcelona_ES'})"
      ],
      "metadata": {
        "id": "BN3mEWOMiNbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For weather"
      ],
      "metadata": {
        "id": "L515d4bZS9Bi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeat all the processes for the Berlin, Paris and Barcelona"
      ],
      "metadata": {
        "id": "fD4ZnZgZjdci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://weatherapi-com.p.rapidapi.com/history.json\"\n",
        "\n",
        "querystring = {\"q\":\"berlin,de,paris,fr,barcellona,sp\",\"dt\":\"2023-09-06\",\"lang\":\"en\",\"end_dt\":\"2023-09-12\"}\n",
        "\n",
        "headers = {\n",
        "\t\"X-RapidAPI-Key\": \"enter-the-key\",\n",
        "\t\"X-RapidAPI-Host\": \"weatherapi-com.p.rapidapi.com\"\n",
        "}\n",
        "\n",
        "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
        "\n",
        "#json\n",
        "JSON(response.json())\n",
        "weather_berlin=response.json()\n",
        "\n",
        "#identify data\n",
        "weather_berlin['list'][0]['main']['temp']\n",
        "weather_berlin['list'][0]['weather'][0]['main']\n",
        "weather_berlin['list'][0]['weather'][0]['description']\n",
        "weather_berlin['list'][0]['dt_txt']\n",
        "\n",
        "#create dataframe\n",
        "day=[]\n",
        "time=[]\n",
        "weather=[]\n",
        "description_weather=[]\n",
        "temp=[]\n",
        "\n",
        "for i in range(len(weather_berlin['list'])):\n",
        "    time.append(weather_berlin['list'][i]['dt_txt'][10:-3])\n",
        "    day.append(weather_berlin['list'][i]['dt_txt'][:10])\n",
        "    weather.append(weather_berlin['list'][i]['weather'][0]['main'])\n",
        "    description_weather.append(weather_berlin['list'][i]['weather'][0]['description'])\n",
        "    temp.append(weather_berlin['list'][0]['main']['temp'])\n",
        "\n",
        "weather_berlin_5_days = pd.DataFrame({'city':'berlin',\n",
        "                                      'weather':weather,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'description':description_weather,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'time':time,'day':day,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'temp':temp,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'city_iso':'berlin_DE'})"
      ],
      "metadata": {
        "id": "etJBb9922mAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paris weather"
      ],
      "metadata": {
        "id": "7IyhU-cRjkzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://weatherapi-com.p.rapidapi.com/history.json\"\n",
        "\n",
        "querystring = {\"q\":\"berlin,de,paris,fr,barcellona,sp\",\"dt\":\"2023-09-06\",\"lang\":\"en\",\"end_dt\":\"2023-09-12\"}\n",
        "\n",
        "headers = {\n",
        "\t\"X-RapidAPI-Key\": \"enter-the-key\",\n",
        "\t\"X-RapidAPI-Host\": \"weatherapi-com.p.rapidapi.com\"\n",
        "}\n",
        "\n",
        "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
        "\n",
        "#json\n",
        "JSON(response.json())\n",
        "weather_paris=response.json()\n",
        "\n",
        "#identify data\n",
        "weather_paris['list'][0]['main']['temp']\n",
        "weather_paris['list'][0]['weather'][0]['main']\n",
        "weather_paris['list'][0]['weather'][0]['description']\n",
        "weather_paris['list'][0]['dt_txt']\n",
        "\n",
        "#create dataframe\n",
        "day=[]\n",
        "time=[]\n",
        "weather=[]\n",
        "description_weather=[]\n",
        "temp=[]\n",
        "\n",
        "for i in range(len(weather_paris['list'])):\n",
        "    time.append(weather_paris['list'][i]['dt_txt'][10:-3])\n",
        "    day.append(weather_paris['list'][i]['dt_txt'][:10])\n",
        "    weather.append(weather_paris['list'][i]['weather'][0]['main'])\n",
        "    description_weather.append(weather_paris['list'][i]['weather'][0]['description'])\n",
        "    temp.append(weather_paris['list'][0]['main']['temp'])\n",
        "\n",
        "weather_paris_5_days = pd.DataFrame({'city':'paris',\n",
        "                                      'weather':weather,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'description':description_weather,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'time':time,'day':day,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'temp':temp,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'city_iso':'paris_FR'})"
      ],
      "metadata": {
        "id": "4xbXGNe0jkTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barcelona weather"
      ],
      "metadata": {
        "id": "--_Bm1Jfj1zE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://weatherapi-com.p.rapidapi.com/history.json\"\n",
        "\n",
        "querystring = {\"q\":\"berlin,de,paris,fr,barcellona,sp\",\"dt\":\"2023-09-06\",\"lang\":\"en\",\"end_dt\":\"2023-09-12\"}\n",
        "\n",
        "headers = {\n",
        "\t\"X-RapidAPI-Key\": \"enter-the-key\",\n",
        "\t\"X-RapidAPI-Host\": \"weatherapi-com.p.rapidapi.com\"\n",
        "}\n",
        "\n",
        "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
        "\n",
        "#json\n",
        "JSON(response.json())\n",
        "weather_barcelona=response.json()\n",
        "\n",
        "#identify data\n",
        "weather_barcelona['list'][0]['main']['temp']\n",
        "weather_barcelona['list'][0]['weather'][0]['main']\n",
        "weather_barcelona['list'][0]['weather'][0]['description']\n",
        "weather_barcelona['list'][0]['dt_txt']\n",
        "\n",
        "#create dataframe\n",
        "day=[]\n",
        "time=[]\n",
        "weather=[]\n",
        "description_weather=[]\n",
        "temp=[]\n",
        "\n",
        "for i in range(len(weather_barcelona['list'])):\n",
        "    time.append(weather_barcelona['list'][i]['dt_txt'][10:-3])\n",
        "    day.append(weather_barcelona['list'][i]['dt_txt'][:10])\n",
        "    weather.append(weather_barcelona['list'][i]['weather'][0]['main'])\n",
        "    description_weather.append(weather_barcelona['list'][i]['weather'][0]['description'])\n",
        "    temp.append(weather_barcelona['list'][0]['main']['temp'])\n",
        "\n",
        "weather_barcelona_5_days = pd.DataFrame({'city':'barcelona',\n",
        "                                      'weather':weather,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'description':description_weather,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'time':time,'day':day,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'temp':temp,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'city_iso':'barcelona_ES'})"
      ],
      "metadata": {
        "id": "hWPG1hSzj3vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After we can create a unique table for all three cities:"
      ],
      "metadata": {
        "id": "eCVnj_kLkEvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weather_forecast_cities=weather_berlin_5_days.append([weather_paris_5_days,weather_barcelona_5_days]).reset_index()\n",
        "weather_forecast_cities=weather_forecast_cities.drop('index', axis=1)"
      ],
      "metadata": {
        "id": "m36zyieEkEYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step two: create a database**\n",
        "\n",
        "So far we only collect data and create dataframes. Now it’s time to create a database and run analysis on MySQL.\n",
        "\n",
        "Open MySQL Workbench, choose local connection, and start writing SQL queries in order to create database. Remember to start with the main table and check if the column we want to use as PRIMARY KEY is also in the other tables and the values are unique to avoid errors. Another important element is the type of values in MYSQL have to be exactly the same in Python."
      ],
      "metadata": {
        "id": "iaTiCG4dkK72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CREATE TABLE IF NOT EXISTS cities(\n",
        "city VARCHAR(255),\n",
        "iso VARCHAR(3),\n",
        "city_iso VARCHAR(255),\n",
        "PRIMARY KEY(city_iso)\n",
        ");"
      ],
      "metadata": {
        "id": "A44jG3SGkJTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, I used as a “main” table cities which is totally useless in terms of analysis, but crucial to connect all the tables.\n",
        "\n",
        "As PRIMARY KEY used city_iso which will be the FOREIGN KEY for population, arrivals, and weather tables."
      ],
      "metadata": {
        "id": "7Pd8nI7jkg4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CREATE DATABASE gans;\n",
        "USE gans;\n",
        "\n",
        "DROP TABLE IF EXISTS cities;\n",
        "CREATE TABLE IF NOT EXISTS cities(\n",
        "city VARCHAR(255),\n",
        "iso VARCHAR(3),\n",
        "city_iso VARCHAR(255),\n",
        "PRIMARY KEY(city_iso)\n",
        ");\n",
        "\n",
        "DROP TABLE IF EXISTS weather_forecast_cities;\n",
        "CREATE TABLE IF NOT EXISTS weather_forecast_cities(\n",
        "weather_id INT AUTO_INCREMENT,\n",
        "city VARCHAR(255),\n",
        "weather VARCHAR(255),\n",
        "description VARCHAR(255),\n",
        "time VARCHAR(255),\n",
        "day VARCHAR(255),\n",
        "temp DECIMAL,\n",
        "city_iso VARCHAR(255),\n",
        "PRIMARY KEY(weather_id),\n",
        "FOREIGN KEY(city_iso) REFERENCES cities(city_iso)\n",
        ");\n",
        "\n",
        "DROP TABLE IF EXISTS arrivals_cities;\n",
        "CREATE TABLE IF NOT EXISTS arrivals_cities(\n",
        "arrivals_id INT AUTO_INCREMENT,\n",
        "city VARCHAR(255),\n",
        "airport_departure VARCHAR(255),\n",
        "airline VARCHAR(255),\n",
        "arrival_time VARCHAR(255),\n",
        "city_iso VARCHAR(255),\n",
        "PRIMARY KEY(arrivals_id),\n",
        "FOREIGN KEY(city_iso) REFERENCES cities(city_iso)\n",
        ");\n",
        "\n",
        "DROP TABLE IF EXISTS population_cities;\n",
        "CREATE TABLE IF NOT EXISTS population_cities(\n",
        "population_id INT AUTO_INCREMENT,\n",
        "age_range VARCHAR(255),\n",
        "value INTEGER,\n",
        "perc INTEGER,\n",
        "city VARCHAR(255),\n",
        "city_iso VARCHAR(255),\n",
        "PRIMARY KEY(population_id),\n",
        "FOREIGN KEY(city_iso) REFERENCES cities(city_iso)\n",
        ");"
      ],
      "metadata": {
        "id": "AUX3dmzgkgpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running SELECT * FROM cities; notice that the tables are empty.\n",
        "\n",
        "Come back to the JupyterNotebook and connect Python to MySQL database.\n",
        "\n",
        "Before, created the connection with local:"
      ],
      "metadata": {
        "id": "neqPKM9mlQcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema=\"gans\"\n",
        "host= host\n",
        "user= root\n",
        "password= password\n",
        "port=3306\n",
        "con = f'mysql+pymysql://{user}:{password}@{host}:{port}/{schema}'"
      ],
      "metadata": {
        "id": "pvoyMn9BlVpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "then run this code to fill the tables:"
      ],
      "metadata": {
        "id": "JPvJAVy4lX_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cities.to_sql('cities', if_exists='append', con=con, index=False)"
      ],
      "metadata": {
        "id": "8aXO14hOlXst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and, in the end, this code to check if works:"
      ],
      "metadata": {
        "id": "y9EMcYeOlc4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_sql(\n",
        "    sql = \"\"\"\n",
        "        select * from cities\n",
        "    \"\"\",\n",
        "    con = con\n",
        ")"
      ],
      "metadata": {
        "id": "73a3_geTlcvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "replicate the process for the other tables:"
      ],
      "metadata": {
        "id": "2vtnEFENlhR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weather_forecast_cities.to_sql('weather_forecast_cities', if_exists='append', con=con, index=False)\n",
        "arrivals_cities.to_sql('arrivals_cities', if_exists='append', con=con, index=False)\n",
        "population_cities.to_sql('population_cities', if_exists='append', con=con, index=False)"
      ],
      "metadata": {
        "id": "1NBP7qkQliOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "if you need a MySQL database filled with your data in Python on the cloud and not on your local machine, we need to do the next steps:\n",
        "\n",
        "We can use AWS(Amazon Web Service) but there are other options like Microsoft Azure and Google Cloud Platform.\n",
        "\n",
        "First of all, create an AWS account, set up an Amazon RDS( Relational Database Service) MySQL Instance, and finally have our AWS connection in MySQL Workbench. To create the database, follow the same process as to create the database on the local machine:"
      ],
      "metadata": {
        "id": "AgQijgtVmDNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CREATE DATABASE gans;\n",
        "USE gans;\n",
        "DROP TABLE IF EXISTS cities;\n",
        "CREATE TABLE IF NOT EXISTS cities(\n",
        "city VARCHAR(255),\n",
        "iso VARCHAR(3),\n",
        "city_iso VARCHAR(255),\n",
        "PRIMARY KEY(city_iso)\n",
        ");\n",
        "DROP TABLE IF EXISTS weather_forecast_cities;\n",
        "CREATE TABLE IF NOT EXISTS weather_forecast_cities(\n",
        "weather_id INT AUTO_INCREMENT,\n",
        "city VARCHAR(255),\n",
        "weather VARCHAR(255),\n",
        "description VARCHAR(255),\n",
        "time VARCHAR(255),\n",
        "day VARCHAR(255),\n",
        "temp DECIMAL,\n",
        "city_iso VARCHAR(255),\n",
        "PRIMARY KEY(weather_id),\n",
        "FOREIGN KEY(city_iso) REFERENCES cities(city_iso)\n",
        ");\n",
        "DROP TABLE IF EXISTS arrivals_cities;\n",
        "CREATE TABLE IF NOT EXISTS arrivals_cities(\n",
        "arrivals_id INT AUTO_INCREMENT,\n",
        "city VARCHAR(255),\n",
        "airport_departure VARCHAR(255),\n",
        "airline VARCHAR(255),\n",
        "arrival_time VARCHAR(255),\n",
        "city_iso VARCHAR(255),\n",
        "PRIMARY KEY(arrivals_id),\n",
        "FOREIGN KEY(city_iso) REFERENCES cities(city_iso)\n",
        ");\n",
        "DROP TABLE IF EXISTS population_cities;\n",
        "CREATE TABLE IF NOT EXISTS population_cities(\n",
        "population_id INT AUTO_INCREMENT,\n",
        "age_range VARCHAR(255),\n",
        "value INTEGER,\n",
        "perc INTEGER,\n",
        "city VARCHAR(255),\n",
        "city_iso VARCHAR(255),\n",
        "PRIMARY KEY(population_id),\n",
        "FOREIGN KEY(city_iso) REFERENCES cities(city_iso)\n",
        ");"
      ],
      "metadata": {
        "id": "8WxslyeYnE8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To move the script into the cloud we need a Lambda function(that it’s different from lambda in Python).\n",
        "\n",
        "Before creating the Lambda function, we will create a “Role”. This role will allow our Lambda function to connect to our RDS instance."
      ],
      "metadata": {
        "id": "BTf9k9AenIj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Create a role\n",
        "Follow the steps below to create the role that will allow your Lambda function to connect to any service.\n",
        "1. Sign in to your AWS console.\n",
        "2. On the search bar at the top, search for \"IAM\" and click on it.\n",
        "3. On the left side menu, click on \"Roles\"\n",
        "4. Click on \"Create role\"\n",
        "5. Select \"AWS service\" as the type of trusted entity.\n",
        "6. Select \"Lambda\" as the use case.\n",
        "7. Click on \"Next: permissions\"\n",
        "8. Tick the box of the policy \"AdministratorAccess\"\n",
        "9. Click on \"Next: Tags\".\n",
        "10. Click on \"Next: Review\"\n",
        "11. Set \"LambdaAdminAccess as the role name.\n",
        "12. Click on \"Create role"
      ],
      "metadata": {
        "id": "bbBXJKwloDHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create a Lambda function.\n",
        "Now it's time to get some Lambda functions done. Follow these steps to create a test function:\n",
        "1.    Sign in to your AWS console.\n",
        "2.    Go to Services > Lambda.\n",
        "3.    Click on \"Create function\".\n",
        "4.    Select \"Author from scratch\"\n",
        "5.    Give your function a name you can recognize. Since we are just testing, we recommend something like \"wbs-test\".\n",
        "6.    On Runtime, select \"Python 3.8\"\n",
        "7.    On \"Permissions\", click on \"Change default execution role\", tick \"Use an existing role and select the \"LambdaAdminAccess\" role we just created.\n",
        "8.    Click on \"Create function\"."
      ],
      "metadata": {
        "id": "FqhAorSao53b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unless for Python, we cannot just install our packages using pip install, import the libraries, and write the code but we need to create the “Layers”, since in our case, the Lambda function it’s like an empty box that we have to fill."
      ],
      "metadata": {
        "id": "5Yn37NKTpLuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The layers can be created in three ways:\n",
        "\n",
        "1. Using the official AWS Data Wrangler following steps\n",
        "2. Uploading pre-compiled packages with KLayers\n",
        "3. Compiling the layers locally and uploading them manually"
      ],
      "metadata": {
        "id": "lpua3CzxpQ9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Come back to the lambda function and import what you need as:"
      ],
      "metadata": {
        "id": "IMf5wtiLpZ-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyown\n",
        "import pymysql.cursors\n",
        "import mysql.connector\n",
        "import sqlalchemy\n",
        "import pymysql"
      ],
      "metadata": {
        "id": "xO91eO9snIYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our first **lambda function** will be the “lambda_handler” since it’s the trigger for the next functions:"
      ],
      "metadata": {
        "id": "SADvwS4SpulS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lambda_handler(event, context):\n",
        "    schema=\"gans\"\n",
        "    host=host\n",
        "    user= user\n",
        "    password=password\n",
        "    port=3306\n",
        "    con = f'mysql+pymysql://{user}:{password}@{host}:{port}/{schema}'\n",
        "    cities data\n",
        "    city=['berlin','paris','barcelona']\n",
        "    iso=['DE','FR','ES']\n",
        "    city_iso=['berlin_DE', 'paris_FR', 'barcelona_ES']\n",
        "    cities=pd.DataFrame({'city':city,\n",
        "                         'iso':iso,\n",
        "                         'city_iso':city_iso})\n",
        "    cities.to_sql('cities', if_exists='append', con=con, index=False)"
      ],
      "metadata": {
        "id": "n0khourhpw8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then define the other functions to call them and fill population_cities, arrivals_cities, weather_forecast_cities:"
      ],
      "metadata": {
        "id": "Psu-c7irp11E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def population(event, context):\n",
        "    schema=\"gans\"\n",
        "    host=host\n",
        "    user=user\n",
        "    password=password\n",
        "    port=3306\n",
        "\n",
        "    con = f'mysql+pymysql://{user}:{password}@{host}:{port}/{schema}'\n",
        "    url = \"https://ugeo.urbistat.com/AdminStat/en/es/demografia/eta/barcelona/8/3\" #population barcelona\n",
        "    response = requests.get(url)\n",
        "    response.status_code\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    table = soup.select_one(\"table.table_result_light\")\n",
        "\n",
        "    age_range = []\n",
        "    for i in range(len(table.select(\"td.table_label\"))):\n",
        "        age_range.append(table.select(\"td.table_label\")[i].get_text())\n",
        "    age_range = pd.DataFrame({\"age_range\": age_range})\n",
        "    age_range.replace(\n",
        "        to_replace=[r\"\\t|\\n|\\r\", \"\\t|\\n|\\r\"], value=[\"\", \"\"], regex=True, inplace=True\n",
        "    )\n",
        "\n",
        "   values = []\n",
        "    for i in range(len(table.select(\"td\"))):\n",
        "        values.append(table.select(\"td\")[i].get_text())\n",
        "\n",
        "    values = pd.DataFrame({\"value\": values})\n",
        "    values.replace(\n",
        "        to_replace=[r\"\\t|\\n|\\r\", \"\\t|\\n|\\r\"], value=[\"\", \"\"], regex=True, inplace=True\n",
        "    )\n",
        "    values.replace(to_replace=[\",\", \"\"], value=[\"\", \"\"], regex=True, inplace=True)\n",
        "    tot_pop = values.loc[[9, 16, 23, 30, 37, 44, 51, 58, 65, 72, 79]]\n",
        "    tot_pop[\"perc\"] = [2, 3, 6, 6, 7, 12, 16, 16, 12, 10, 9]\n",
        "    tot_pop[\"city\"] = \"barcelona\"\n",
        "    tot_pop = tot_pop.reset_index()\n",
        "    tot_pop = tot_pop.drop(\"index\", axis=1)\n",
        "\n",
        "    population_barcelona = age_range.join(tot_pop, how=\"inner\")\n",
        "    population_barcelona[\"city_iso\"] = \"barcelona_ES\"\n",
        "\n",
        "\n",
        "    url = \"https://ugeo.urbistat.com/AdminStat/en/de/demografia/eta/berlin/11/3\" #population berlin\n",
        "    response = requests.get(url)\n",
        "    response.status_code\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    table = soup.select_one(\"table.table_result_light\")\n",
        "\n",
        "    age_range = []\n",
        "    for i in range(len(table.select(\"td.table_label\"))):\n",
        "        age_range.append(table.select(\"td.table_label\")[i].get_text())\n",
        "    age_range = pd.DataFrame({\"age_range\": age_range})\n",
        "    age_range.replace(\n",
        "        to_replace=[r\"\\t|\\n|\\r\", \"\\t|\\n|\\r\"], value=[\"\", \"\"], regex=True, inplace=True\n",
        "    )\n",
        "\n",
        "    values = []\n",
        "    for i in range(len(table.select(\"td\"))):\n",
        "        values.append(table.select(\"td\")[i].get_text())\n",
        "\n",
        "    values = pd.DataFrame({\"value\": values})\n",
        "    values.replace(\n",
        "        to_replace=[r\"\\t|\\n|\\r\", \"\\t|\\n|\\r\"], value=[\"\", \"\"], regex=True, inplace=True\n",
        "    )\n",
        "    values.replace(to_replace=[\",\", \"\"], value=[\"\", \"\"], regex=True, inplace=True)\n",
        "    tot_pop = values.loc[[9, 16, 23, 30, 37, 44, 51, 58, 65, 72, 79]]\n",
        "    tot_pop[\"perc\"] = [2, 3, 6, 6, 7, 12, 16, 16, 12, 10, 9]\n",
        "    tot_pop[\"city\"] = \"berlin\"\n",
        "    tot_pop = tot_pop.reset_index()\n",
        "    tot_pop = tot_pop.drop(\"index\", axis=1)\n",
        "\n",
        "    population_berlin=age_range.join(tot_pop, how=\"inner\")\n",
        "    population_berlin['city_iso']= 'berlin_DE'\n",
        "\n",
        "\n",
        "    url = \"https://ugeo.urbistat.com/AdminStat/en/fr/demografia/eta/paris/75/3\" #population paris\n",
        "    response = requests.get(url)\n",
        "    response.status_code\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    table = soup.select_one(\"table.table_result_light\")\n",
        "\n",
        "    age_range = []\n",
        "    for i in range(len(table.select(\"td.table_label\"))):\n",
        "        age_range.append(table.select(\"td.table_label\")[i].get_text())\n",
        "    age_range = pd.DataFrame({\"age_range\": age_range})\n",
        "    age_range.replace(\n",
        "        to_replace=[r\"\\t|\\n|\\r\", \"\\t|\\n|\\r\"], value=[\"\", \"\"], regex=True, inplace=True\n",
        "    )\n",
        "\n",
        "    values = []\n",
        "    for i in range(len(table.select(\"td\"))):\n",
        "        values.append(table.select(\"td\")[i].get_text())\n",
        "    values = pd.DataFrame({\"value\": values})\n",
        "    values.replace(\n",
        "        to_replace=[r\"\\t|\\n|\\r\", \"\\t|\\n|\\r\"], value=[\"\", \"\"], regex=True, inplace=True\n",
        "    )\n",
        "    values.replace(to_replace=[\",\", \"\"], value=[\"\", \"\"], regex=True, inplace=True)\n",
        "\n",
        "    tot_pop = values.loc[[9, 16, 23, 30, 37, 44, 51, 58, 65, 72, 79]]\n",
        "    tot_pop[\"perc\"] = [2, 3, 6, 6, 7, 12, 16, 16, 12, 10, 9]\n",
        "    tot_pop[\"city\"] = \"paris\"\n",
        "    tot_pop = tot_pop.reset_index()\n",
        "    tot_pop = tot_pop.drop(\"index\", axis=1)\n",
        "\n",
        "    population_paris=age_range.join(tot_pop, how=\"inner\")\n",
        "    population_paris['city_iso']= 'paris_FR'\n",
        "\n",
        "    population_cities=population_paris.append([population_berlin,population_barcelona]).reset_index() #population cities\n",
        "    population_cities=population_cities.drop('index', axis=1)\n",
        "\n",
        "    population_cities['value']=population_cities['value'].astype(int)\n",
        "    population_cities.to_sql('population_cities', if_exists='append', con=con, index=False) #population cities table"
      ],
      "metadata": {
        "id": "jU_ZQaJsp2cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tips to avoid errors:\n",
        "\n",
        "Set time runout at 15 minutes\n",
        "Make sure you have all you need on the layers\n",
        "Check if your code is indented well\n",
        "Delete everything don’t need in your code"
      ],
      "metadata": {
        "id": "PVQg9EyIqc_r"
      }
    }
  ]
}