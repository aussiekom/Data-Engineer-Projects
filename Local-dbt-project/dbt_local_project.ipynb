{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "dbt (data build tool) is one of the hottest technologies in the data engineering and analytics space."
      ],
      "metadata": {
        "id": "CDMTlQo5yvop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Create a dbt project\n",
        "We will be populating some data in a Postgres database therefore, we first need to install the dbt Postgres adapter from PyPI:"
      ],
      "metadata": {
        "id": "-vQS0btzimh7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ESKCUNCUQfG"
      },
      "outputs": [],
      "source": [
        "pip install dbt-postgres==1.3.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the command will also install the dbt-core package as well as other dependencies that are required for running dbt.\n",
        "\n",
        "Now let’s go ahead and create a dbt project — to do so, we can initialise a new dbt project by running the dbt init command in the terminal:"
      ],
      "metadata": {
        "id": "Uuidp2btisuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dbt init test_dbt_project"
      ],
      "metadata": {
        "id": "f_D_eUyniviw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we will be prompted to select which database we like to use (depending on the adapters we have installed locally, we may see different options)"
      ],
      "metadata": {
        "id": "ZCZR3FYPixAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Create a Docker Compose file**"
      ],
      "metadata": {
        "id": "MnAnrW6weEKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s create a docker-compose.yml file (place the file at the same level as the test_dbt_projectdirectory) in which we will be specifying two services — one would correspond to a ready-made Postgres image and the second one to a dbt image that we will define in a Dockerfile in the next step:"
      ],
      "metadata": {
        "id": "QBG8nqVreGAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "version: \"3.9\"\n",
        "\n",
        "services:\n",
        "  postgres:\n",
        "    container_name: postgres\n",
        "    image: frantiseks/postgres-sakila\n",
        "    ports:\n",
        "      - '5432:5432'\n",
        "    healthcheck:\n",
        "      test: [\"CMD-SHELL\", \"pg_isready -U postgres\"]\n",
        "      interval: 5s\n",
        "      timeout: 5s\n",
        "      retries: 5\n",
        "  dbt:\n",
        "    container_name: dbt\n",
        "    build: .\n",
        "    image: dbt-dummy\n",
        "    volumes:\n",
        "      - ./:/usr/src/dbt\n",
        "    depends_on:\n",
        "      postgres:\n",
        "        condition: service_healthy"
      ],
      "metadata": {
        "id": "AoRK8n9Ii2cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can tell, for the Postgres container, we will be using an image called frantiseks/postgres-sakila which is publicly available and accessible on Docker Hub. This image, will populate the Sakila Database on the Postgres instance. The database models a DVD rental store and is consisted of multiple tables which are normalised and correspond to entities such as films, actors, customers and payments. In the next few following sections we’ll make use of this data in order to build some example dbt data models.\n",
        "\n",
        "The second service, called dbt, will be the one that creates an environment where we will build our data models. Note that we mount the current directory into the docker container. This will let the container have access to any changes we may be doing to the data models without having to re-build the image. Additionally, any metadata generated by dbt commands (such as manifet.json) will appear instantly on the host machine."
      ],
      "metadata": {
        "id": "4PqQSp6ueXHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Create a Dockerfile.**\n",
        "\n",
        "Now let’s specify a Dockerfile that will be used to build an image on top of which the running container will then build the models specified in our example dbt project."
      ],
      "metadata": {
        "id": "JUATXABReaFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FROM python:3.10-slim-buster\n",
        "\n",
        "RUN apt-get update \\\n",
        "    && apt-get install -y --no-install-recommends\n",
        "\n",
        "WORKDIR /usr/src/dbt/dbt_project\n",
        "\n",
        "# Install the dbt Postgres adapter. This step will also install dbt-core\n",
        "RUN pip install --upgrade pip\n",
        "RUN pip install dbt-postgres==1.3.1\n",
        "\n",
        "# Install dbt dependencies (as specified in packages.yml file)\n",
        "# Build seeds, models and snapshots (and run tests wherever applicable)\n",
        "CMD dbt deps && dbt build --profiles-dir profiles && sleep infinity"
      ],
      "metadata": {
        "id": "Ua_MRzSteZDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in the last CMD command, we intentionally added an extra && sleep infinity command such that the container won’t exit after running the steps specified in the Dockerfile so that we can then access the container and run additional dbt commands (if needed)."
      ],
      "metadata": {
        "id": "Mdpujjwoeerk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Create a dbt profile for the Postgres database.**\n",
        "\n",
        "Now that we have created the required infrastructure for our host machines in order to create a Postgres database, populate some dummy data as well as creating an image for our dbt environment, let’s focus on the dbt side.\n",
        "\n",
        "We will first have to create a dbt profile that will be used when interacting with the target Postgres database. Within the test_dbt_project directory, create another directory called profiles and then a file called profiles.yml with the following content:"
      ],
      "metadata": {
        "id": "7XE36AOIefd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_profile:\n",
        "  target: dev\n",
        "  outputs:\n",
        "    dev:\n",
        "      type: postgres\n",
        "      host: postgres\n",
        "      user: postgres\n",
        "      password: postgres\n",
        "      port: 5432\n",
        "      dbname: postgres\n",
        "      schema: public\n",
        "      threads: 1"
      ],
      "metadata": {
        "id": "kyClXrVReh_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Define some data models.**\n",
        "\n",
        "The next step is to create some data models based on the Sakila data populated by the Postgres container. If you are planning to use this project for testing purposes, I would advise to create at least one seed, one model and a snapshot (with tests if possible) so that you have a full coverage of all dbt entities (macros excluding)."
      ],
      "metadata": {
        "id": "_vBNQfStqqVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Run the Docker containers.**\n",
        "\n",
        "We now have everything we need in order to spin up the two docker containers we specified in the docker-compose.yml file earlier, and build the data models defined in our example dbt project.\n",
        "\n",
        "First, let’s build the images"
      ],
      "metadata": {
        "id": "L8P1qczxqtRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docker-compose build"
      ],
      "metadata": {
        "id": "240I6UDLqvVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now let’s spin up the running containers:"
      ],
      "metadata": {
        "id": "IpXnCZ3vqwtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docker-compose up"
      ],
      "metadata": {
        "id": "lfFDIK36qyN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command should have initialised a Postgres database using the Sakila Database, and created the dbt models specified. For now, let’s make sure you have two running containers:"
      ],
      "metadata": {
        "id": "hYxwl7LgqzQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docker ps"
      ],
      "metadata": {
        "id": "17OMXP0Wq0_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "should give an output that includes one container with name dbt and another one with name *postgres*."
      ],
      "metadata": {
        "id": "nHDa8mkkrXK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7: Query the models on Postgres database.**\n",
        "\n",
        "In order to access the Postgres container, you’ll first need to infer the container id"
      ],
      "metadata": {
        "id": "qqVfSrQjrX9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docker ps"
      ],
      "metadata": {
        "id": "OQODswdarVlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then run"
      ],
      "metadata": {
        "id": "-saxfBPRr0vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docker exec -it <container-id> /bin/bash"
      ],
      "metadata": {
        "id": "aNs22ABWr60l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will then need to use psql, a command-line interface that gives us access the postgres instance:"
      ],
      "metadata": {
        "id": "wTgC6phKr1qU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psql -U postgres"
      ],
      "metadata": {
        "id": "lC_8YVyPr9ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have used the data models I’ve shared in the previous sections, you can now query each of the models created on Postgres using the queries below."
      ],
      "metadata": {
        "id": "-esnr_0xr-oS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "-- Query seed tables\n",
        "SELECT * FROM customer_base;\n",
        "\n",
        "-- Query staging views\n",
        "SELECT * FROM stg_payment;\n",
        "\n",
        "-- Query intermediate views\n",
        "SELECT * FROM int_customers_per_store;\n",
        "SELECT * FROM int_revenue_by_date;\n",
        "\n",
        "-- Query mart tables\n",
        "SELECT * FROM cumulative_revenue;\n",
        "\n",
        "-- Query snapshot tables\n",
        "SELECT * FROM int_stock_balances_daily_grouped_by_day_snapshot;"
      ],
      "metadata": {
        "id": "Ln0EXoiPsAGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8: Creating additional or modifying existing models**\n",
        "\n",
        "\n",
        "As mentioned already, the Dockerfile and docker-compose.yml files were written in such a way such that the dbt container would still be up and running. Therefore, whenever you modify or create data models, you can still use that container to re-build seeds, models, snapshots and/or tests.\n",
        "\n",
        "To do so, first infer the container id of the dbt container:"
      ],
      "metadata": {
        "id": "eHZ8EnTpsBiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docker ps"
      ],
      "metadata": {
        "id": "-ZT2rYQ2sFHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then enter the running container by running"
      ],
      "metadata": {
        "id": "2VXQO8vxsGHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docker exec -it <container-id> /bin/bash"
      ],
      "metadata": {
        "id": "BfY9QhMwsHLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally run any dbt command you wish, depending on the modifications you’ve made to the example dbt project. Here’s a quick reference of the most commonly used commands for these purposes:"
      ],
      "metadata": {
        "id": "uY-2arYJsIIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dbt deps\n",
        "dbt deps\n",
        "\n",
        "# Build seeds\n",
        "dbt seeds --profiles-dir profiles\n",
        "\n",
        "# Build data models\n",
        "dbt run --profiles-dir profiles\n",
        "\n",
        "# Build snapshots\n",
        "dbt snapshot --profiles-dir profiles\n",
        "\n",
        "# Run tests\n",
        "dbt test --profiles-dir profiles"
      ],
      "metadata": {
        "id": "wHM3eGOUsJvw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}